{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.clear_data import FileDeleter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning download...\n",
      "Starting pariza/bbc-news-summary Download\n",
      "Completed pariza/bbc-news-summary Download\n",
      "Starting rmisra/news-category-dataset Download\n",
      "Completed rmisra/news-category-dataset Download\n",
      "Download completed!\n",
      "json converted to DataFrame!\n",
      "Downloading articles...\n",
      "Articles downloaded!\n",
      "Moving and renaming files...\n",
      "Shutil task completed!\n"
     ]
    }
   ],
   "source": [
    "%run scripts/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted directory: data/processed/test\n",
      "Deleted directory: data/processed/train\n",
      "Deleted directory: data/processed/val\n"
     ]
    }
   ],
   "source": [
    "fd = FileDeleter()\n",
    "fd.delete_all_in_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\19105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\19105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\19105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\19105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\19105\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(document):\n",
    "    sentences = sent_tokenize(document)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sents):\n",
    "    sentences_processed = []\n",
    "    for sentence in sents:\n",
    "        sentence_reduced = sentence.replace(\"[^a-zA-Z0-9_]\", '')\n",
    "        sentence_reduced = [word.lower() for word in sentence_reduced.split(' ') if word.lower() not in stopwords.words('english')]\n",
    "        sentences_processed.append(' '.join(word for word in sentence_reduced))\n",
    "    return sentences_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentences, vectorizer_type='count'):\n",
    "    if vectorizer_type == 'count':\n",
    "        # Get vocabulary for entire document\n",
    "        sentences = [sent.split(' ') for sent in sentences]\n",
    "        all_words = list(set([word for s in sentences for word in s]))\n",
    "\n",
    "        # Create feature vector for each sentence\n",
    "        feature_vecs = []\n",
    "        for sentence in sentences:\n",
    "            feature_vec = [0] * len(all_words)\n",
    "            for word in sentence:\n",
    "                feature_vec[all_words.index(word)] += 1\n",
    "            feature_vecs.append(feature_vec)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        feature_vecs = vectorizer.fit_transform(sentences)\n",
    "        feature_vecs = feature_vecs.todense().tolist()\n",
    "        \n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency_matrix(feature_vecs):\n",
    "    # Create empty adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(feature_vecs), len(feature_vecs)))\n",
    " \n",
    "    # Populate the adjacency matrix using the similarity of all pairs of sentences\n",
    "    for i in range(len(feature_vecs)):\n",
    "        for j in range(len(feature_vecs)):\n",
    "            if i == j: #ignore if both are the same sentence\n",
    "                continue \n",
    "            adjacency_matrix[i][j] = 1 - cosine_distance(feature_vecs[1], feature_vecs[j])\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sentences,adjacency_matrix,top_n):\n",
    "\n",
    "    # Create the graph representing the document\n",
    "    document_graph = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "    # Apply PageRank algorithm to get centrality scores for each node/sentence\n",
    "    scores = nx.pagerank(document_graph)\n",
    "    scores_list = list(scores.values())\n",
    "\n",
    "    # Sort and pick top sentences\n",
    "    ranking_idx = np.argsort(scores_list)[::-1]\n",
    "    ranked_sentences = [sentences[i] for i in ranking_idx]   \n",
    "\n",
    "    summary = []\n",
    "    for i in range(top_n):\n",
    "        summary.append(ranked_sentences[i])\n",
    "\n",
    "    summary = \" \".join(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.build_features import BuildFeatures\n",
    "\n",
    "bf = BuildFeatures()\n",
    "datasets = bf.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_extracted = get_sentences(test_text)\n",
    "sentences_processed = preprocess(sentences_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market benchmarks in Europe and Asia fell by as much as 4% as traders tried to figure out how large Putin’s incursion would be and the scale of Western retaliation. In early trading, the FTSE 100 in London fell 2.5% to 7,311.69 as Europe awakened to news of explosions in the Ukrainian capital of Kyiv, the major city of Kharkiv and other areas. The euro fell to $1.1243 from $1.1306. India’s Sensex fell 3.4% to 55,283.65. President Joe Biden denounced the attack as “unprovoked and unjustified” and said Moscow would be held accountable, which many took to mean Washington and its allies would impose additional sanctions.\n"
     ]
    }
   ],
   "source": [
    "feature_vecs = vectorize(sentences_processed,vectorizer_type='count')\n",
    "adjacency_matrix = generate_adjacency_matrix(feature_vecs)\n",
    "summary = summarize(sentences_extracted,adjacency_matrix,top_n=5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = train_data[0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = rouge.compute(predictions=[summary], references=[test_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.0588235294117647,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.04411764705882353,\n",
       " 'rougeLsum': 0.04411764705882353}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
